[
  {
    "questionNumber": 1,
    "question": "What are the challenges associated with using the Tanh(x) activation function?",
    "options": [
      "It is not zero centered",
      "Computationally expensive",
      "No n-differentiable at 0",
      "Saturation"
    ],
    "attemptedAnswer": ["Saturation", "Computationally expensive"]
  },
  {
    "questionNumber": 2,
    "question": "Which of the following problems makes training a neural network harder while using sigmoid as the activation function?",
    "options": [
      "Not-continuous at 0",
      "Not-differentiable at 0",
      "Saturation",
      "Computationally expensive"
    ],
    "attemptedAnswer": ["Saturation","Computationally expensive"]
  },
  {
    "questionNumber": 3,
    "question": "Consider the Exponential ReLU (ELU) activation function. Which of the following statements is true?",
    "options": [
      "The function is discontinuous at x = 0",
      "The function is non-differentiable at x = 0",
      "Exponential ReLU can produce negative values",
      "Exponential ReLU is computationally less expensive than ReLU"
    ],
    "attemptedAnswer": "Exponential ReLU can produce negative values"
  },
  {
    "questionNumber": 4,
    "question": "We have observed that the sigmoid neuron has become saturated. What might be the possible output values at this neuron?",
    "options": [
      "0.0666",
      "0.589",
      "0.9734",
      "0.498",
      "1"
    ],
    "attemptedAnswer": ["0.0666","0.9734","1"]
  },
  {
    "questionNumber": 5,
    "question": "What is the gradient of the sigmoid function at saturation?",
    "options": [
      "0"
    ],
    "attemptedAnswer": "0"
  },
  {
    "questionNumber": 6,
    "question": "Which of the following are common issues caused by saturating neurons in deep networks?",
    "options": [
      "Vanishing gradients",
      "Slow convergence during training",
      "Overfitting",
      "Increased model complexity"
    ],
    "attemptedAnswer": ["Vanishing gradients","Slow convergence during training"]
  },
  {
    "questionNumber": 7,
    "question": "Given a neuron initialized with weights w1 = 0.9, w2 = 1.7 and inputs x1 = 0.4, x2 = -0.7, calculate the output of a ReLU neuron.",
    "options": [
      "0"
    ],
    "attemptedAnswer": "0"
  },
  {
    "questionNumber": 8,
    "question": "Which of the following is incorrect with respect to the batch normalization process in neural networks?",
    "options": [
      "We normalize the output produced at each layer before feeding it into the next layer",
      "Batch normalization leads to a better initialization of weights.",
      "Backpropagation can be used after batch normalization",
      "Variance and mean are not learnable parameters."
    ],
    "attemptedAnswer": "Variance and mean are not learnable parameters."
  },
  {
    "questionNumber": 9,
    "question": "Which of the following is an advantage of unsupervised pre-training in deep learning?",
    "options": [
      "It helps in reducing overfitting",
      "Pre-trained models converge faster",
      "It requires fewer computational resources",
      "It improves the accuracy of the model"
    ],
    "attemptedAnswer": ["It helps in reducing overfitting","Pre-trained models converge faster","It improves the accuracy of the model"]
  },
  {
    "questionNumber": 10,
    "question": "How can you tell if your network is suffering from the Dead ReLU problem?",
    "options": [
      "The loss function is not decreasing during training",
      "A large number of neurons have zero output",
      "The accuracy of the network is not improving",
      "The network is overfitting to the training data"
    ],
    "attemptedAnswer": "A large number of neurons have zero output"
  }
]
